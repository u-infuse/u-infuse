{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Infuse: RetinaNet Auto-Annotator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Author:** Andrew Shepley\n",
    "* **Contact:** asheple2@une.edu.au (alternate contact: andreashepley01@gmail.com)\n",
    "* **Source:** U-Infuse\n",
    "* **Purpose:**\n",
    " * choose multi-class or single class auto-annotation\n",
    " * generate 1 xml file per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import *\n",
    "from preprocessing import *\n",
    "from auto_annotation import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# use this environment flag to change which GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows all datasets in ../datasets/. You can only annotate images that are in ../datasets \n",
    "get_all_datasets('../datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose one of the folders from list above\n",
    "folder = 'FiN_rhino/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose multi-class or not\n",
    "multi_class = False\n",
    "model_dir = './pretrained_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if multi-class is False, ALWAYS use the default retinanet foreground object detector. \n",
    "chosen_model = 'single_class_annotator.h5'\n",
    "\n",
    "#link classes set below to user textbox (class, e.g. cow) - only for single class\n",
    "classes = {\"kangaroo\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If multi_class is True run this cell. It shows which models are available for annotation.\n",
    "get_all_files(model_dir, 'h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If multi_class is True run this cell. Choose one of the models from list above\n",
    "chosen_model = 'my_first_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run whether multi_class is true or false\n",
    "model = choose_inference_model(model_dir, chosen_model)\n",
    "class_mapping_file = os.path.join(model_dir,chosen_model[:-2]+\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the relevant class mapping file based on user's chosen class\n",
    "write_class_file(class_mapping_file, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set confidence threshold. Max is 100, min is 0\n",
    "user_selected_confidence = 50\n",
    "confidence_threshold = set_confidence_threshold(user_selected_confidence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unannotated_dataset = os.path.join('../datasets',folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the list of images that will be passed into the annotation model\n",
    "images_to_annotate = generate_list_of_images(unannotated_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#annotate all images in images_to_annotate\n",
    "for each_im in images_to_annotate:\n",
    "    #perform object detection \n",
    "    processed_image, object_detection_data = inference_per_image(unannotated_dataset, model, each_im, class_mapping_file, confidence_threshold)\n",
    "    #create an annotation object => use class RetinaNet_Auto_Annotator()\n",
    "    annotation = RetinaNet_Auto_Annotator(unannotated_dataset, each_im, object_detection_data, processed_image.shape, confidence_threshold)\n",
    "    #write the xml file for this image\n",
    "    annotation.write_xml_file()\n",
    "  \n",
    "    #display images inline\n",
    "    img = Image.fromarray(processed_image, 'RGB')\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GUI ONLY.Process one image at a time. SAMPLE.\n",
    "image_0 = images_to_annotate[0]\n",
    "#perform object detection \n",
    "processed_image, object_detection_data = inference_per_image(unannotated_dataset, model, image_0, class_mapping_file, confidence_threshold)\n",
    "#create an annotation object = use class RetinaNet_Auto_Annotator()\n",
    "annotation_0 = RetinaNet_Auto_Annotator(unannotated_dataset, image_0, object_detection_data, processed_image.shape,confidence_threshold)\n",
    "#write the xml file for this image\n",
    "annotation_0.write_xml_file()\n",
    "\n",
    "#displays image\n",
    "img = Image.fromarray(processed_image, 'RGB')\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
